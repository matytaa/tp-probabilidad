#!/usr/bin/env python
# coding: utf-8

# In[ ]:


Parte 1: Simulación

En esta primera parte, construiremos varios generadores de números aleatorios que usaremos para obtener muestras con distribu-
ción conocida sobre las que vamos a trabajar posteriormente.

1. Utilizando únicamente la función random de su lenguaje (la función que genera un número aleatorio uniforme entre 0 y 1),
implemente una función que genere un número distribuido Bernoulli con probabilidad p.
2. Utilizando la función del punto anterior, implemente otra que genere un número binomial con los parámetros n,p.

3. Utilizando el procedimiento descrito en el capítulo 6 del Dekking (método de la función inversa o de Monte Carlo), imple-
mentar una función que permita generar un número aleatorio con distribución E xp(λ).

4. Investigar como generar números aleatorios con distribución normal. Implementarlo.

Parte 2: Estadística descriptiva
Ahora vamos a aplicar las técnicas vistas en la materia al estudio de algunas muestras de datos.
1. Generar tres muestras de números aleatorios Exp(0,5) de tamaño n = 10, n = 30 y n = 200. Para cada una, computar la media
get_ipython().set_next_input('y varianza muestral. ¿Qué observa');get_ipython().run_line_magic('pinfo', 'observa')
2. Para las tres muestras anteriores, graficar los histogramas de frecuencias relativas con anchos de banda 0,4, 0,2 y 0,1; es decir,
get_ipython().set_next_input('un total de 9 histogramas. ¿Qué conclusiones puede obtener');get_ipython().run_line_magic('pinfo', 'obtener')
3. Generar una muestra de números Bin(10, 0,3) de tamaño n = 50. Construir la función de distribución empírica de dicha
muestra.

4. A partir de la función de distribución empírica del punto anterior, generar una nueva muestra de números aleatorios utili-
zando el método de simulación de la primera parte. Computar la media y varianza muestral y graficar el histograma.

5. Repetir el experimento de los dos puntos anteriores con dos muestras aleatorias más generadas con los mismos parámetros.
get_ipython().set_next_input('¿Qué conclusión saca');get_ipython().run_line_magic('pinfo', 'saca')
Parte 3: Convergencia
El propósito de esta sección es ver en forma práctica los resultados de los teoremas de convergencia.
1. Generar cuatro muestras de números aleatorios de tamaño 100, todas con distribución binomial con p = 0,40 y n = 10, n = 20,
get_ipython().set_next_input('n = 50 y n = 100 respectivamente. Graficar sus histogramas. ¿Qué observa');get_ipython().run_line_magic('pinfo', 'observa')
2. Elija la muestra de tamaño 200 y calcule la media y desviación estándar muestral. Luego, normalice cada dato de la muestra
y grafique el histograma de la muestra normalizada. Justifique lo que observa.
3. Para cada una de las muestras anteriores, calcule la media muestral. Justifique lo que observa.

Parte 4: Estadística inferencial
Para terminar, vamos a hacer inferencia con las muestras que generamos y obtener así información sobre sus distribuciones.
1. Generar dos muestras N(100, 5), una de tamaño n = 10 y otra de tamaño n = 30. Obtener estimaciones puntuales de su media
y varianza.
2. Suponga que ya conoce el dato de que la distribución tiene varianza 5. Obtener intervalos de confianza del 95% y 98% para
la media de ambas muestras.
3. Repita el punto anterior pero usando la varianza estimada s
2
para("la", "muestra", "de", "tamaño", "adecuado.")
4. Probar a nivel 0,99 la hipótesis de que la varianza sea σ
2 > 5. Calcular la probabilidad de cometer error tipo II para la hipótesis
alternativa σ
2 = 6.
5. Agrupando los datos en subgrupos de longitud 0,5, probar a nivel 0,99 la hipótesis de que la muestra proviene de una distribución normal.


# In[3]:


#Imports que voy a necesitar

import numpy as np # importando numpy
from scipy import stats # importando scipy.stats
import pandas as pd # importando pandas
import matplotlib.pyplot as plt # importando matplotlib
import seaborn as sns # importando seaborn
# importanto la api de statsmodels
import statsmodels.formula.api as smf
import statsmodels.api as sm
import math
import collections 
from statsmodels.distributions.empirical_distribution import ECDF


# In[2]:


# Bibliografia
# https://relopezbriega.github.io/blog/2016/06/29/distribuciones-de-probabilidad-con-python
# https://relopezbriega.github.io/blog/2015/06/27/probabilidad-y-estadistica-con-python/
# Fn Binomial: https://www.aglarick.com/2020/02/15/generacion-de-la-distribucion-binomial-en-python-con-jupyter-y-matplotlib/
# Fn Normal: https://stackoverrun.com/es/q/3329235


# # Parte 1: Simulación

# # Ejercicio 1

# In[3]:


# En esta primera parte, construiremos varios generadores de números aleatorios que usaremos para obtener muestras con distribu-
# ción conocida sobre las que vamos a trabajar posteriormente.

# 1. Utilizando únicamente la función random de su lenguaje (la función que genera un número aleatorio uniforme entre 0 y 1),
# implemente una función que genere un número distribuido Bernoulli con probabilidad p.


# In[4]:


#Fijo la semilla del random para que siempre sean los mismos datos 
np.random.seed(1)


# In[5]:


# np.random.uniform(0,1) "la función que genera un número aleatorio uniforme entre 0 y 1"
# De esta forma, devuelve valores equiprobables entre 0 y 1
# Los valores, los voy a generar de esta forma valor = np.random.uniform(0,1) y dps, comparo con el valor de p (prob)

def fn_bernoulli_random(p):
        if np.random.uniform(0,1) > p:
            return 0
        else:
            return 1


# In[6]:


# implemente una función que genere un array de valores distribuido Bernoulli con probabilidad p.
def fn_bernoulli_array(x,p):
    valores = np.zeros((x))
    for i in range(0,x):
        valores[i-1] = fn_bernoulli_random(p)
    return valores
        


# In[7]:


val = fn_bernoulli_array(100,0.4)
print(val)
collections.Counter(val) 


# In[8]:


# La Distribución de Bernoulli describe un experimento probabilístico en donde el ensayo 
# tiene dos posibles resultados, éxito o fracaso.

# p   es la probabilidad de éxito
# 1−p es la probabilidad de fracaso

bernoulli = stats.bernoulli(p)

# Función de Masa de Probabilidad
fmp = bernoulli.pmf(x) 

# Graficando Bernoulli
fig, ax = plt.subplots()
ax.plot(x, fmp, 'bo')
ax.vlines(x, 0, fmp, colors='b', lw=5, alpha=0.5)
ax.set_yticks([0., 0.2, 0.4, 0.6])

plt.title('Distribución Bernoulli')
plt.ylabel('probabilidad')
plt.xlabel('valores')
plt.show()


# # Ejercicio 2

# 2. Utilizando la función del punto anterior, implemente otra que genere un número binomial con los parámetros n,p.

# In[29]:


def fn_binomial_random(n,p):
    intentos=[np.random.uniform(0,1) for x in range(0,n)]
    exitos=[intento<=p for intento in intentos]
    return sum(exitos)


# In[30]:


val = fn_binomial_random(50,0.4)
print(val)


# In[31]:


# Implemento funcion para retornar un conjunto de valores aleatorios siguiendo la distribucion binomial
def fn_binomial_array(casos,n,p):
    valores = np.zeros(casos)
    for i in range(0,casos):
        valores[i-1] = fn_binomial_random(n,p)
    return valores
        


# In[32]:


val = fn_binomial_array(1000,6,0.5)
print(val)
collections.Counter(val) 


# In[33]:


def graficaBinomial(puntostotales, n, p):
    Xs=[k/n    for k in range(0,n+1)]
    Ys=[0    for i in range(0,n+1)]
    puntoactual=0
    while puntoactual<puntostotales:
        ubicacion=fn_binomial_random(n, p)
        Ys[ubicacion]+=1
        puntoactual+=1
    return Xs, Ys

ns=[6]
p=0.5
puntos=1000
for n in ns:
    curva=graficaBinomial(puntos, n, p)
    plt.plot(*curva, label=f'n = {n}')
plt.xlabel('Probabilidad ')
plt.ylabel('')
plt.legend()
plt.show()


# # Ejercicio 3

# In[ ]:


Utilizando el procedimiento descrito en el capítulo 6 del Dekking (método de la función inversa o de Monte Carlo), imple-
mentar una función que permita generar un número aleatorio con distribución Exp(λ).


# Metodo de funcion inversa: pagina 74 del Dekkings

# ![image.png](attachment:image.png)

# In[15]:


def fn_inversa_exponencial(_lambda, u):
    return -(1/_lambda)* math.log10(u) 


# In[16]:


def fn_exponencial_random(_lambda):
    numeroRandomConDistribucionUniforme = np.random.uniform(0,1)
    result = fn_inversa_exponencial(_lambda, numeroRandomConDistribucionUniforme)
    return result


# In[17]:


val = fn_exponencial_random(0.5)
print(val)


# # Ejercicio 4

# In[ ]:


Investigar como generar números aleatorios con distribución normal. Implementarlo.


# Usamos el método de Box-muller para generar numeros random siguiendo una distribucion normal 
# https://es.wikipedia.org/wiki/M%C3%A9todo_de_Box-Muller

# In[18]:


def fn_gaussian_random(mean, stddev): 
    theta = 2 * math.pi * np.random.uniform(0,1) 
    rho = math.sqrt(-2 * math.log10(1 - np.random.uniform(0,1))) 
    scale = stddev * rho 
    x = mean + scale * math.cos(theta) 
    y = mean + scale * math.sin(theta) 
    return y


# In[19]:


val = fn_gaussian_random(2, 3)
print(val)


# In[89]:


def fn_normal_array(casos,mean,stddev):
    valores = np.zeros(casos)
    for i in range(0,casos):
        valores[i-1] = fn_gaussian_random(mean, stddev)
    return valores


# In[90]:


val = fn_normal_array(10,100, 5)
print(val)


# # Parte 2: Estadística descriptiva

# In[ ]:


Ahora vamos a aplicar las técnicas vistas en la materia al estudio de algunas muestras de datos.


# # Ejercicio 1

# In[ ]:


Generar tres muestras de números aleatorios Exp(0,5) de tamaño n = 10, n = 30 y n = 200. Para cada una, computar la media
get_ipython().set_next_input('y varianza muestral. ¿Qué observa');get_ipython().run_line_magic('pinfo', 'observa')


# In[20]:


def fn_generador_de_muestras_numeros_random_con_dist_exponencial(n,_lambda):
    valores = np.zeros(n)
    for i in range(0,n):
        valores[i-1] = fn_exponencial_random(_lambda)
    
    #Calculo de esperanza
    suma = 0
    for i in range(0,10):
        suma = suma + valores[i]

    esperanza = suma/10

    #Calculo de varianza
    suma = 0
    for i in range(0,10):
        suma = suma + math.pow(valores[i], 2)

    varianza = suma/10 - math.pow(esperanza, 2)

    return valores, esperanza, varianza


# Primera muestra con n=10: 

# In[21]:


val_n10, esperanza, varianza = fn_generador_de_muestras_numeros_random_con_dist_exponencial(10,0.5)
print(val_n10)
collections.Counter(val_n10) 
print("Media=", esperanza)
print("Varianza=", varianza)


# Segunda muestra con n=30: 

# In[22]:


val_n30, esperanza, varianza = fn_generador_de_muestras_numeros_random_con_dist_exponencial(30,0.5)
print(val_n30)
collections.Counter(val_n30) 
print("Media=", esperanza)
print("Varianza=", varianza)


# In[25]:


val_n200, esperanza, varianza = fn_generador_de_muestras_numeros_random_con_dist_exponencial(200,0.5)
print(val_n200)
collections.Counter(val_n200) 
print("Media=", esperanza)
print("Varianza=", varianza)


# # Ejercicio 2

# In[23]:


Para las tres muestras anteriores, graficar los histogramas de frecuencias relativas con anchos de banda 0,4, 0,2 y 0,1; es decir,
get_ipython().set_next_input('un total de 9 histogramas. ¿Qué conclusiones puede obtener');get_ipython().run_line_magic('pinfo', 'obtener')


# In[26]:


inicio = int(min(val_n10))
fin = int(max(val_n10))
print("Inicio=", inicio)
print("Fin=", fin)
ancho = 0.4
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_n10,div)
plt.grid()
plt.show()
ancho = 0.2
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_n10,div)
plt.grid()
plt.show()
ancho = 0.1
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_n10,div)
plt.grid()
plt.show()


# In[80]:


inicio = int(min(val_n30))
fin = int(max(val_n30))
ancho = 0.4
print("Inicio=", inicio)
print("Fin=", fin)
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_n30,div)
plt.grid()
plt.show()
ancho = 0.2
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_n30,div)
plt.grid()
plt.show()
ancho = 0.1
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_n30,div)
plt.grid()
plt.show()


# In[27]:


inicio = int(min(val_n200))
fin = int(max(val_n200))
ancho = 0.4
print("Inicio=", inicio)
print("Fin=", fin)
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_n200,div)
plt.grid()
plt.show()
ancho = 0.2
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_n200,div)
plt.grid()
plt.show()
ancho = 0.1
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_n200,div)
plt.grid()
plt.show()


# # Ejercicio 3

# In[ ]:


# Generar una muestra de números Bin(10, 0.3) de tamaño de muestra N = 50. Construir la función de distribución empírica de dicha
# muestra.


# In[34]:


val_e3 = fn_binomial_array(50,10,0.3)
print(val_e3)
collections.Counter(val_e3)


# In[35]:


ecdf = ECDF(val_e3)
print('P(x<0): %.3f' % ecdf(0))
print('P(x<1): %.3f' % ecdf(1))
print('P(x<2): %.3f' % ecdf(2))
print('P(x<3): %.3f' % ecdf(3))
print('P(x<4): %.3f' % ecdf(4))
print('P(x<5): %.3f' % ecdf(5))
print('P(x<6): %.3f' % ecdf(6))
# plot the cdf
plt.plot(ecdf.x, ecdf.y)
plt.show()


# # Ejercicio 4

# In[ ]:


A partir de la función de distribución empírica del punto anterior, generar una nueva muestra de números aleatorios utili-
zando el método de simulación de la primera parte. 
Computar la media y varianza muestral y graficar el histograma.


# In[36]:


inicio = int(min(val_e3))
fin = int(max(val_e3))
ancho = 0.4
print("Inicio=", inicio)
print("Fin=", fin)
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_e3,div)
plt.grid()
plt.show()


# In[ ]:


# Referencia Funcion de Distribucion Empirica
# https://machinelearningmastery.com/empirical-distribution-function-in-python/#:~:text=An%20empirical%20distribution%20function%20can,specific%20observations%20from%20the%20domain.


# In[54]:


# esperanza = n*p (binomial)
esperanza = 10 * 0.3
print(esperanza)


# In[56]:


# varianza = n*p*(1-p)
varianza = 10*0.3*(1-0.3)
print(varianza)


# # Ejercicio 5

# In[ ]:


Repetir el experimento de los dos puntos anteriores con dos muestras aleatorias más generadas con los mismos parámetros.
get_ipython().set_next_input('¿Qué conclusión saca');get_ipython().run_line_magic('pinfo', 'saca')


# # Parte 3: Convergencia

# # Ejercicio 1

# In[ ]:


# Generar cuatro muestras de números aleatorios de tamaño 100, todas con distribución binomial con p = 0,40 y n = 10, n = 20,
# n = 50 y n = 100 respectivamente. Graficar sus histogramas. ¿Qué observa?


# In[38]:


val_e1_p3_10 = fn_binomial_array(100,10,0.4)
print(val_e1_p3_10)
collections.Counter(val_e1_p3_10)


# In[39]:


val_e1_p3_20 = fn_binomial_array(100,20,0.4)
print(val_e1_p3_20)
collections.Counter(val_e1_p3_20)


# In[40]:


val_e1_p3_50 = fn_binomial_array(100,50,0.4)
print(val_e1_p3_50)
collections.Counter(val_e1_p3_50)


# In[41]:


val_e1_p3_100 = fn_binomial_array(100,100,0.4)
print(val_e1_p3_100)
collections.Counter(val_e1_p3_100)


# In[48]:


inicio = int(min(val_e1_p3_10))
fin = int(max(val_e1_p3_10))
print("Inicio=", inicio)
print("Fin=", fin)
print("Valor pico de frecuencia=", 10*0.4)
ancho = 0.4
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_e1_p3_10,div)
plt.grid()
plt.show()


# In[49]:


inicio = int(min(val_e1_p3_20))
fin = int(max(val_e1_p3_20))
print("Inicio=", inicio)
print("Fin=", fin)
print("Valor pico de frecuencia=", 20*0.4)
ancho = 0.4
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_e1_p3_20,div)
plt.grid()
plt.show()


# In[50]:


inicio = int(min(val_e1_p3_50))
fin = int(max(val_e1_p3_50))
print("Inicio=", inicio)
print("Fin=", fin)
print("Valor pico de frecuencia=", 50*0.4)
ancho = 0.4
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_e1_p3_50,div)
plt.grid()
plt.show()


# In[51]:


inicio = int(min(val_e1_p3_100))
fin = int(max(val_e1_p3_100))
print("Inicio=", inicio)
print("Fin=", fin)
print("Valor pico de frecuencia=", 100*0.4)
ancho = 0.4
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_e1_p3_100,div)
plt.grid()
plt.show()


# In[ ]:


# CONCLUSION: Teniendo en cuenta el p=0.4, puedo estimar que el pico de frencuencia en cada uno, va a estar determinado por el 
# valor del rango que sea valor_maximo*0.4 (aproximadamente)


# # Ejercicio 2

# In[52]:


# Elija la muestra de tamaño 200 y calcule la media y desviación estándar muestral. Luego, normalice cada dato de la muestra
# y grafique el histograma de la muestra normalizada. Justifique lo que observa.


# In[53]:


val_e2_p3_200 = fn_binomial_array(200,10,0.4)
print(val_e2_p3_200)
collections.Counter(val_e2_p3_200)


# In[57]:


# esperanza = n*p (binomial)
esperanza = 10 * 0.4
print(esperanza)


# In[58]:


# varianza = n*p*(1-p)
varianza = 10*0.4*(1-0.4)
print(varianza)


# In[59]:


inicio = int(min(val_e2_p3_200))
fin = int(max(val_e2_p3_200))
print("Inicio=", inicio)
print("Fin=", fin)
print("Valor pico de frecuencia=", esperanza)
ancho = 0.4
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(val_e2_p3_200,div)
plt.grid()
plt.show()


# In[77]:


# Para normalizar el valor, tomo la siguiente normalización = ( x – min(x) ) / ( max(x) – min(x) )

minimo = (min(val_e2_p3_200))
maximo = (max(val_e2_p3_200))

calculo_b = maximo - minimo

normal = np.zeros((200))

for i in range(0,200):
    valor = val_e2_p3_200[i-1]
    calculo_a = valor - minimo
    calculo = calculo_a / calculo_b
    normal[i-1] = calculo

print(normal)


# In[78]:


df = pd.DataFrame({"valor":val_e2_p3_200,"normalizado":normal})


# In[79]:


print(df)


# In[86]:


inicio = int(min(normal))
fin = int(max(normal))
print("Inicio=", inicio)
print("Fin=", fin)

ancho = 0.05
div = np.linspace(inicio,fin,round(1+(fin-inicio)/ancho))
plt.hist(normal,div)
plt.grid()
plt.show()


# In[85]:


# En el primer conjunto, la esperanza de la binomial, me da que en el valor 4 tendre el pico de frencuencia de ocurrencia
# Cuando normalizo los valores a un intervalo {0,1}, el valor representado de 4 es 0,5 esto en el histograma
# me muestra el valor con mayor frecuencia de ocurrencia.


# # Ejercicio 3

# In[87]:


# Para cada una de las muestras anteriores, calcule la media muestral. Justifique lo que observa.


# In[ ]:





# # Parte 4

# # Ejercicio 1

# In[88]:


# Generar dos muestras N(100, 5), una de tamaño n = 10 y otra de tamaño n = 30. Obtener estimaciones puntuales de su media
# y varianza.


# In[91]:


val_e1_p4_10 = fn_normal_array(10,100, 5)
print(val_e1_p4_10)


# In[92]:


val_e1_p4_30 = fn_normal_array(30,100, 5)
print(val_e1_p4_30)


# In[101]:


suma_a = 0
for i in range(0,10):
    suma_a = suma_a + val_e1_p4_10[i-1]


# In[102]:


print(suma_a/10)


# In[103]:


varianza_a = 0
for i in range(0,10):
    varianza_a = varianza_a + (val_e1_p4_10[i-1]-suma_a)**2


# In[104]:


print(varianza_a/10)


# In[106]:


suma_b = 0
for i in range(0,30):
    suma_b = suma_b + val_e1_p4_30[i-1]


# In[107]:


print(suma_b/30)


# In[108]:


varianza_b = 0
for i in range(0,30):
    varianza_b = varianza_b + (val_e1_p4_30[i-1]-suma_b)**2


# In[109]:


print(varianza_b/30)


# In[ ]:




